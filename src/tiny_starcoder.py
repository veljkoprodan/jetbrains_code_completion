# -*- coding: utf-8 -*-
"""tiny_starcoder.ipynb

Automatically generated by Colab.

"""

#!huggingface-cli login

from google.colab import drive
drive.mount('/content/drive')

from transformers import AutoModelForCausalLM, AutoTokenizer
import torch

access_token = 'secret'

device = 'cuda' if torch.cuda.is_available() else 'cpu'
model = AutoModelForCausalLM.from_pretrained("bigcode/tiny_starcoder_py", token=access_token).to(device)
tokenizer = AutoTokenizer.from_pretrained("bigcode/tiny_starcoder_py", padding_side='left', token=access_token)

def generate_code(prefix, suffix):
  input_text = "<fim_prefix>" + prefix + "<fim_suffix>" + suffix + "<fim_middle>"

  model.config.pad_token_id = tokenizer.eos_token_id
  tokenizer.pad_token = tokenizer.eos_token

  inputs = tokenizer(input_text,
                    return_tensors="pt",
                    padding=True,
                    truncation=True,
                    max_length=512,
                  ).to(device)


  outputs = model.generate(**inputs,
                           max_new_tokens=144,
                           do_sample=True,
                           temperature=0.2,
                           top_k=50,
                           top_p=0.1,
                           repetition_penalty=1.17)

  return outputs

import os
import json

with open('/content/drive/MyDrive/code_completion/dataset.json', 'r') as file:
  dataset = json.load(file)

prefixes = [data['prefix'] for data in dataset]
suffixes = [data['suffix'] for data in dataset]

decoded_outputs = []

for prefix, suffix in zip(prefixes, suffixes):
  outputs = generate_code(prefix, suffix)
  decoded_outputs.append(tokenizer.decode(outputs[0], skip_special_tokens=False))

for output, data in zip(decoded_outputs, dataset):
  start_idx = output.find("<fim_middle>") + len("<fim_middle>")
  end_idx = output.find("<|endoftext|>")

  if start_idx == -1:
    print('Error')
    break

  if end_idx == -1:
    data['middle_predicted'] = output[start_idx:]
  else:
    data['middle_predicted'] = output[start_idx:end_idx]

with open('/content/drive/MyDrive/code_completion/raw_outputs.json', 'w') as file:
  json.dump(decoded_outputs, file)

with open('/content/drive/MyDrive/code_completion/dataset_output.json', 'w') as file:
  json.dump(dataset, file)
